{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f75420bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>content</th>\n",
       "      <th>url</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Beyond GPT-4: What's New?</td>\n",
       "      <td>LLM Variants and Meta's Open Source Before she...</td>\n",
       "      <td>https://pub.towardsai.net/beyond-gpt-4-whats-n...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Building a Q&amp;A Bot over Private Documents with...</td>\n",
       "      <td>Private data to be used The example provided c...</td>\n",
       "      <td>https://pub.towardsai.net/building-a-q-a-bot-o...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Enhancing E-commerce Product Search Using LLMs</td>\n",
       "      <td>Problem Statement Despite the pioneers like Am...</td>\n",
       "      <td>https://pub.towardsai.net/enhancing-e-commerce...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Exploring Large Language Models -Part 3</td>\n",
       "      <td>Fine Tuning on Custom Domain Data All the popu...</td>\n",
       "      <td>https://pub.towardsai.net/exploring-large-lang...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Fine-Tuning a Llama-2 7B Model for Python Code...</td>\n",
       "      <td>New Llama-2 model In mid-July, Meta released i...</td>\n",
       "      <td>https://pub.towardsai.net/fine-tuning-a-llama-...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Foundation Models: Scaling Large Language Models</td>\n",
       "      <td>New Moore's Laws Achieving Zettascale Computin...</td>\n",
       "      <td>https://pub.towardsai.net/foundation-models-37...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>GPTQ Quantization on a Llama 2 7B Fine-Tuned M...</td>\n",
       "      <td>GPTQ: Post-training quantization on generative...</td>\n",
       "      <td>https://pub.towardsai.net/gptq-quantization-on...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>LLaMA by Meta leaked by an anonymous forum: Qu...</td>\n",
       "      <td>LLaMA: Meta's new AI tool According to the off...</td>\n",
       "      <td>https://pub.towardsai.net/llama-by-meta-leaked...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>LLaMA-GPT4All: Simplified Local ChatGPT</td>\n",
       "      <td>Introduce GPT4All GPT4All is a large language ...</td>\n",
       "      <td>https://pub.towardsai.net/llama-gpt4all-simpli...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Inside Code Llama: Meta AI's Entrance in the C...</td>\n",
       "      <td>Inside Code Llama The release of Code Llama do...</td>\n",
       "      <td>https://pub.towardsai.net/inside-code-llama-me...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Meta's Llama 2: Revolutionizing Open Source La...</td>\n",
       "      <td>I. Llama 2: Revolutionizing Commercial Use Unl...</td>\n",
       "      <td>https://pub.towardsai.net/metas-llama-2-revolu...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>The Generative AI Revolution: Exploring the Cu...</td>\n",
       "      <td>What is Generative AI? Generative AI is a subf...</td>\n",
       "      <td>https://pub.towardsai.net/the-generative-ai-re...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>Building Intuition on the Concepts behind LLMs...</td>\n",
       "      <td>Neural Networks LLMs like ChatGPT are trained ...</td>\n",
       "      <td>https://pub.towardsai.net/building-intuition-o...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>WizardCoder: Why It's the Best Coding Model Ou...</td>\n",
       "      <td>What Sets WizardCoder Apart One might wonder w...</td>\n",
       "      <td>https://pub.towardsai.net/wizardcoder-why-its-...</td>\n",
       "      <td>towards_ai</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                title  \\\n",
       "0                           Beyond GPT-4: What's New?   \n",
       "1   Building a Q&A Bot over Private Documents with...   \n",
       "2      Enhancing E-commerce Product Search Using LLMs   \n",
       "3             Exploring Large Language Models -Part 3   \n",
       "4   Fine-Tuning a Llama-2 7B Model for Python Code...   \n",
       "5    Foundation Models: Scaling Large Language Models   \n",
       "6   GPTQ Quantization on a Llama 2 7B Fine-Tuned M...   \n",
       "7   LLaMA by Meta leaked by an anonymous forum: Qu...   \n",
       "8             LLaMA-GPT4All: Simplified Local ChatGPT   \n",
       "9   Inside Code Llama: Meta AI's Entrance in the C...   \n",
       "10  Meta's Llama 2: Revolutionizing Open Source La...   \n",
       "11  The Generative AI Revolution: Exploring the Cu...   \n",
       "12  Building Intuition on the Concepts behind LLMs...   \n",
       "13  WizardCoder: Why It's the Best Coding Model Ou...   \n",
       "\n",
       "                                              content  \\\n",
       "0   LLM Variants and Meta's Open Source Before she...   \n",
       "1   Private data to be used The example provided c...   \n",
       "2   Problem Statement Despite the pioneers like Am...   \n",
       "3   Fine Tuning on Custom Domain Data All the popu...   \n",
       "4   New Llama-2 model In mid-July, Meta released i...   \n",
       "5   New Moore's Laws Achieving Zettascale Computin...   \n",
       "6   GPTQ: Post-training quantization on generative...   \n",
       "7   LLaMA: Meta's new AI tool According to the off...   \n",
       "8   Introduce GPT4All GPT4All is a large language ...   \n",
       "9   Inside Code Llama The release of Code Llama do...   \n",
       "10  I. Llama 2: Revolutionizing Commercial Use Unl...   \n",
       "11  What is Generative AI? Generative AI is a subf...   \n",
       "12  Neural Networks LLMs like ChatGPT are trained ...   \n",
       "13  What Sets WizardCoder Apart One might wonder w...   \n",
       "\n",
       "                                                  url      source  \n",
       "0   https://pub.towardsai.net/beyond-gpt-4-whats-n...  towards_ai  \n",
       "1   https://pub.towardsai.net/building-a-q-a-bot-o...  towards_ai  \n",
       "2   https://pub.towardsai.net/enhancing-e-commerce...  towards_ai  \n",
       "3   https://pub.towardsai.net/exploring-large-lang...  towards_ai  \n",
       "4   https://pub.towardsai.net/fine-tuning-a-llama-...  towards_ai  \n",
       "5   https://pub.towardsai.net/foundation-models-37...  towards_ai  \n",
       "6   https://pub.towardsai.net/gptq-quantization-on...  towards_ai  \n",
       "7   https://pub.towardsai.net/llama-by-meta-leaked...  towards_ai  \n",
       "8   https://pub.towardsai.net/llama-gpt4all-simpli...  towards_ai  \n",
       "9   https://pub.towardsai.net/inside-code-llama-me...  towards_ai  \n",
       "10  https://pub.towardsai.net/metas-llama-2-revolu...  towards_ai  \n",
       "11  https://pub.towardsai.net/the-generative-ai-re...  towards_ai  \n",
       "12  https://pub.towardsai.net/building-intuition-o...  towards_ai  \n",
       "13  https://pub.towardsai.net/wizardcoder-why-its-...  towards_ai  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Downloading and loading the mini blog dataset\n",
    "url = \"https://raw.githubusercontent.com/AlaFalaki/tutorial_notebooks/main/data/mini-llama-articles.csv\"\n",
    "mini_dataset = pd.read_csv(url)\n",
    "\n",
    "# Displaying the first few entries (run in a Jupyter Notebook)\n",
    "mini_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20d40de9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'LLM Variants and Meta\\'s Open Source Before shedding light on four major trends, I\\'d share the latest Meta\\'s Llama 2 and Code Llama. Meta\\'s Llama 2 represents a sophisticated evolution in LLMs. This suite spans models pretrained and fine-tuned across a parameter spectrum of 7 billion to 70 billion. A specialized derivative, Llama 2-Chat, has been engineered explicitly for dialogue-centric applications. Benchmarking revealed Llama 2\\'s superior performance over most extant open-source chat models. Human-centric evaluations, focusing on safety and utility metrics, positioned Llama 2-Chat as a potential contender against proprietary, closed-source counterparts. The development trajectory of Llama 2 emphasized rigorous fine-tuning methodologies. Meta\\'s transparent delineation of these processes aims to catalyze community-driven advancements in LLMs, underscoring a commitment to collaborative and responsible AI development. Code Llama is built on top of Llama 2 and is available in three models: Code Llama, the foundational code model;Codel Llama - Python specialized for Python;and Code Llama - Instruct, which is fine-tuned for understanding natural language instructions. Based on its benchmark testing, Code Llama outperformed state-of-the-art publicly available LLMs (except GPT-4) on code tasks. Llama 2, Llama 2-Chat, and Code Llama are key steps in LLM development but still have a way to go compared to GPT-4. Meta\\'s open access and commitment to improving these models promise transparent and faster LLM progress in the future. Please refer to the LLM and Llama variants below:  From LLMs to Multimodal LLMs, like OpenAI\\'s ChatGPT (GPT-3.5), primarily focus on understanding and generating human language. They\\'ve been instrumental in tasks like text generation, translation, and even creative writing. However, their scope is limited to text. Enter multimodal models like GPT-4. These are a new breed of AI models that can understand and generate not just text, but also images, sounds, and potentially other types of data. The term \"multimodal\" refers to their ability to process multiple modes or types of data simultaneously. This is a game-changer. Imagine an AI that can not only read a description of a dress but also visualize it or even design it! Multimodal AI models are moving us towards more holistic AI systems. These systems can potentially understand our world in a more comprehensive manner, bridging the gap between different forms of data and providing richer, more integrated solutions. As we stand on the cusp of this new era, it\\'s exciting to envision the myriad of applications and innovations that Multimodal models will bring to the table. The future of AI looks more integrated and versatile than ever before.  From Connections to Vector DB The AI landscape is witnessing a fascinating transition: from Language Model (LLM) connections or integrations, e.g., LangChain and LlamaIndex, to the rise of Vector Databases (Vector DB) such as Weaviate, Milvus, Pinecone, Chroma, and Vespa.ai. But what\\'s driving this shift, and why does it matter? LLM connections, like the LlamaIndex, primarily focus on linking and understanding vast amounts of external data. They\\'ve been pivotal in creating semantic connections, enabling more intuitive search experiences, and enhancing data accessibility. However, as the volume and variety of data grow, the need for more advanced storage and retrieval mechanisms becomes evident. This is where Vector DBs come into play. Unlike traditional databases that store data in rows and columns, Vector DBs store data in high-dimensional space, allowing for more efficient and accurate similarity searches. Tools like Weaviate and Milvus are designed to handle massive datasets, making them ideal for tasks like image recognition, recommendation systems, and more. The rise of Vector DBs represents a broader trend in AI: the quest for more efficient, scalable, and versatile data handling solutions. As we navigate this evolution, it\\'s clear that the combination of LLMs and Vector DBs will redefine how we store, access, and understand data in the AI-driven future.  From Agents to OS The AI realm is abuzz with innovations, and one of the most intriguing shifts we\\'re witnessing is the transition from LLM agents to using LLMs as Operating Systems (OS). Let\\'s delve into this evolution and its implications. LLM agents, like AutoGPT, AgentGPT, BabyAGI, and HuggingGPT, have been groundbreaking in automating tasks based on user requests. These agents leverage the power of Language Models (LLMs) to understand and execute commands, making them invaluable in tasks ranging from content generation to data analysis. Their adaptability and intelligence have made them a staple in many AI toolkits. However, the vision for AI doesn\\'t stop there. The concept of LLM as an OS is emerging as the next big thing. Imagine an operating system where the core is a language model, orchestrating everything around it. Such a system would not just execute tasks but would understand context, anticipate needs, and offer solutions in real time. It\\'s like turning the LLM into the brain of the digital ecosystem, making devices and applications more intuitive and responsive than ever. The move towards LLM as OS signifies a paradigm shift in how we perceive and utilize AI. It\\'s not just about automation anymore; it\\'s about creating a seamless, intelligent interface between humans and technology. As we stand on the brink of this transformation, the potential for LLM-driven OS to revolutionize our digital interactions is immense.  From Fine-tuning to Plugins The world of LLMs is undergoing a transformative shift, moving from intricate fine-tuning processes to the more dynamic realm of plugins. Let\\'s unpack this evolution. Historically, fine-tuning has been the cornerstone of LLM optimization. There are two primary ways to fine-tune LLMs: feeding data into the LLM in real-time and directly fine-tuning on the LLM. From a technical standpoint, this involves three methods: Transfer Learning: Adapting a pre-trained model to new tasks.Sequential Fine-tuning: Refining models in stages for specific tasks.Task-specific Fine-tuning: Tailoring models for a particular function. Moreover, LLM techniques like In-context learning, Few-shot learning, and Zero-shot learning have further enhanced the model\\'s adaptability, allowing them to understand and generate content with minimal data. However, the future of LLMs is leaning towards plugins. With the introduction of tools like GPT-4 Plugins, the focus is on extending LLMs seamlessly. Instead of running LLMs as a service, they\\'re envisioned as platforms. This means integrating LLMs with various tools, enhancing their capabilities, and offering a more modular and scalable approach to AI applications. The journey from fine-tuning to plugins represents a move from static optimization to dynamic adaptability, ensuring that LLMs remain at the forefront of AI innovation.  In a Nutshell The AI domain is witnessing rapid shifts, with LLMs playing a central role. Initially, the move was from LLMs to Multimodal models, expanding from text to include images and sounds. Simultaneously, the trend shifted from LLM connections, which linked external data, to Vector Databases for efficient high-dimensional storage. Another evolution saw LLM agents, which automated tasks, transitioning towards LLMs as Operating Systems. This change aims for more intuitive, context-aware devices and applications. Furthermore, the traditional fine-tuning processes of LLMs are now being replaced by dynamic plugins, turning LLMs into platforms integrated with various tools. Leading this LLM revolution are OpenAI\\'s GPT-4 and Meta\\'s LLaMA2. Their pioneering efforts are setting the stage for an AI future that\\'s more integrated, responsive, and attuned to human interactions.  More Readings Harnessing the Power of LLMs in Practice: A Survey on ChatGPT and Beyond: https://arxiv.org/abs/2304.13712Sparks of Artificial General Intelligence: Early experiments with GPT-4: https://arxiv.org/abs/2303.12712GPT4All-J: https://huggingface.co/nomic-ai/gpt4all-jIntroducing Code Llama, a state-of-the-art large language model for coding: https://ai.meta.com/blog/code-llama-large-language-model-coding/Llama 2: Open Foundation and Fine-Tuned Chat Models: https://ai.meta.com/research/publications/llama-2-open-foundation-and-fine-tuned-chat-models/'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mini_dataset.iloc[0].content"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
